# -*- coding: utf-8 -*-
"""KDD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19CEXTj5oe8yTqBxJKzgBnq7B8SLF3cIs

Importing the Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""Reading Datasets"""

train_df = pd.read_csv('KDDTrain+.csv')
test_df=pd.read_csv('KDDTest+.csv')

columns = (['duration'
,'protocol_type'
,'service'
,'flag'
,'src_bytes'
,'dst_bytes'
,'land'
,'wrong_fragment'
,'urgent'
,'hot'
,'num_failed_logins'
,'logged_in'
,'num_compromised'
,'root_shell'
,'su_attempted'
,'num_root'
,'num_file_creations'
,'num_shells'
,'num_access_files'
,'num_outbound_cmds'
,'is_host_login'
,'is_guest_login'
,'count'
,'srv_count'
,'serror_rate'
,'srv_serror_rate'
,'rerror_rate'
,'srv_rerror_rate'
,'same_srv_rate'
,'diff_srv_rate'
,'srv_diff_host_rate'
,'dst_host_count'
,'dst_host_srv_count'
,'dst_host_same_srv_rate'
,'dst_host_diff_srv_rate'
,'dst_host_same_src_port_rate'
,'dst_host_srv_diff_host_rate'
,'dst_host_serror_rate'
,'dst_host_srv_serror_rate'
,'dst_host_rerror_rate'
,'dst_host_srv_rerror_rate'
,'attack'
,'level'])

train_df.columns = columns
test_df.columns = columns
train_df

"""Preprocessing"""

print("No. of Null Values : ",train_df.isna().sum().sum())
duplicates = len(train_df[train_df.duplicated()])
print("The No. of Duplicates :  ",duplicates)

train_df.dtypes

train_df.nunique()

train_df.var()

train_df.drop(["num_outbound_cmds"],axis=1,inplace=True) ## since variance is  0 we are dropping that column
test_df.drop(["num_outbound_cmds"],axis=1,inplace=True) ## since variance is  0 we are dropping that column

"""Correlation"""

labels=['id', 'duration', 'protocol_type', 'service', 'flag',
       'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent',
       'hot', 'num_failed_logins', 'logged_in', 'num_compromised',
       'root_shell', 'su_attempted', 'num_root', 'num_file_creations',
       'num_shells', 'num_access_files', 'is_host_login',
       'is_guest_login', 'count', 'srv_count', 'serror_rate',
       'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',
       'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate',
       'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',
       'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',
       'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',
       'dst_host_srv_serror_rate', 'dst_host_rerror_rate',
       'dst_host_srv_rerror_rate', 'class']
import seaborn as sns
plt.figure(figsize=(30,30))
cor = train_df.corr()
plt.figure(figsize = (50,40))
sns.heatmap(cor, annot=True, cmap=plt.cm.CMRmap_r,xticklabels=labels,yticklabels=labels)
plt.show()

cor_matrix = train_df.corr().abs()
upper_tri = cor_matrix.where(np.triu(np.ones(cor_matrix.shape),k=1).astype(np.bool))
to_drop = [column for column in upper_tri.columns if any(upper_tri[column] >= 0.9)]
to_drop

test_df.drop(['num_root',
           'srv_serror_rate',
           'srv_rerror_rate',
           'dst_host_serror_rate',
           'dst_host_srv_serror_rate',
           'dst_host_rerror_rate',
           'dst_host_srv_rerror_rate'], axis=1,inplace= True)
train_df.drop(['num_root',
           'srv_serror_rate',
           'srv_rerror_rate',
           'dst_host_serror_rate',
           'dst_host_srv_serror_rate',
           'dst_host_rerror_rate',
           'dst_host_srv_rerror_rate'], axis=1,inplace= True) ## dropped because of highly correlated features

## the unique columns from service are = ['harvest', 'http_2784', 'red_i', 'http_8001', 'urh_i', 'aol']

train_df.drop(train_df.index[train_df['service'] == 'http_2784'], inplace = True)
train_df.drop(train_df.index[train_df['service'] == 'red_i'], inplace = True)
train_df.drop(train_df.index[train_df['service'] == 'http_8001'], inplace = True)
train_df.drop(train_df.index[train_df['service'] == 'urh_i'], inplace = True)
train_df.drop(train_df.index[train_df['service'] == 'aol'], inplace = True)
train_df.drop(train_df.index[train_df['service'] == 'harvest'], inplace = True)

"""Mapping attacks"""

train_df.attack.unique()

train_df['is_attack']=train_df['attack'].map(lambda a : 0 if a == 'normal' else 1)
test_df['is_attack']=test_df['attack'].map(lambda a : 0 if a == 'normal' else 1)

train_df.replace(to_replace =['apache2', 'back', 'land', 'mailbomb', 'neptune', 'pod', 'processtable', 'smurf', 'teardrop', 'udpstorm', 'worm'], 
                            value ="dos",inplace=True)

train_df.replace(to_replace =['ftp_write', 'guess_passwd', 'httptunnel', 'phf', 'imap', 'multihop', 'named', 'sendmail', 'snmpgetattack', 'snmpguess', 'spy', 'warezclient', 'warezmaster', 'xlock', 'xsnoop'], 
                            value ="r2l",inplace=True)

train_df.replace(to_replace =['ipsweep', 'portsweep', 'mscan', 'nmap', 'saint', 'satan'], 
                            value ="probe",inplace=True)

train_df.replace(to_replace =['buffer_overflow', 'loadmodule', 'perl', 'ps', 'sqlattack', 'rootkit', 'xterm'], 
                            value ="u2r",inplace=True)

test_df.replace(to_replace =['apache2', 'back', 'land', 'mailbomb', 'neptune', 'pod', 'processtable', 'smurf', 'teardrop', 'udpstorm', 'worm'
], 
                            value ="dos",inplace=True)

test_df.replace(to_replace =['ftp_write', 'guess_passwd', 'httptunnel', 'phf', 'imap', 'multihop', 'named', 'sendmail', 'snmpgetattack', 'snmpguess', 'spy', 'warezclient', 'warezmaster', 'xlock', 'xsnoop'], 
                            value ="r2l",inplace=True)

test_df.replace(to_replace =['ipsweep', 'portsweep', 'mscan', 'nmap', 'saint', 'satan'], 
                            value ="probe",inplace=True)

test_df.replace(to_replace =['buffer_overflow', 'loadmodule', 'perl', 'ps', 'sqlattack', 'rootkit', 'xterm'], 
                            value ="u2r",inplace=True)

train_df

apriori_df=pd.concat([train_df,test_df]).drop_duplicates().reset_index(drop=True)

apriori_df = apriori_df[['protocol_type', 'service', 'flag','attack']].copy()
apriori_df

di = {"normal" : 0,"dos" :1, "probe" :2, "r2l" :3 ,"u2r" :4} ## dictionary
train_df["attack"].replace(di, inplace=True)
test_df["attack"].replace(di, inplace=True)

train_df.attack.unique()

"""Checking the impact of each independent feature on Y"""

(train_df.corrwith(train_df["attack"]).sort_values(ascending=False))

test_df.drop(['hot',
           'wrong_fragment',
           'is_guest_login',
           'num_failed_logins',
           'src_bytes',
           'srv_diff_host_rate',
           'dst_bytes','root_shell','num_shells','urgent','land','is_host_login','num_compromised','num_file_creations','su_attempted','num_access_files','srv_count'], axis=1,inplace= True)
train_df.drop(['hot',
           'wrong_fragment',
           'is_guest_login',
           'num_failed_logins',
           'src_bytes',
           'srv_diff_host_rate',
           'dst_bytes','root_shell','num_shells','urgent','land','is_host_login','num_compromised','num_file_creations','su_attempted','num_access_files','srv_count'], axis=1,inplace= True)

"""Encoding"""

protocol_type=pd.get_dummies(train_df.protocol_type)
merged=pd.concat([train_df,protocol_type],axis='columns')
train_df=merged.drop(['protocol_type','icmp'],axis=1)

flag=pd.get_dummies(train_df.flag)
merged=pd.concat([train_df,flag],axis='columns')
train_df=merged.drop(['flag','SF'],axis=1)

service=pd.get_dummies(train_df.service)
merged=pd.concat([train_df,service],axis='columns')
train_df=merged.drop(['service','other'],axis=1)


protocol_type=pd.get_dummies(test_df.protocol_type)
merged=pd.concat([test_df,protocol_type],axis='columns')
test_df=merged.drop(['protocol_type','icmp'],axis=1)

flag=pd.get_dummies(test_df.flag)
merged=pd.concat([test_df,flag],axis='columns')
test_df=merged.drop(['flag','SF'],axis=1)

service=pd.get_dummies(test_df.service)
merged=pd.concat([test_df,service],axis='columns')
test_df=merged.drop(['service','other'],axis=1)

col = train_df.pop('attack')
train_df.insert(90,'attack',col,allow_duplicates=False)
col = test_df.pop('attack')
test_df.insert(90,'attack',col,allow_duplicates=False)

x_train=train_df.iloc[:,:-1].values
y_train=train_df.iloc[:,-1].values
x_test=test_df.iloc[:,:-1].values
y_test=test_df.iloc[:,-1].values

"""Model Prediciton And Building"""

train_df

from sklearn.preprocessing import StandardScaler
sc= StandardScaler()
x_train[:,0:14]= sc.fit_transform(x_train[:,0:14])
x_test[:,0:14]= sc.transform(x_test[:,0:14])

"""# KNN"""

from sklearn.neighbors import KNeighborsClassifier
classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
classifier.fit(x_train, y_train)

y_pred = classifier.predict(x_test)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""# Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
dtc = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
dtc.fit(x_train, y_train)

y_pred = dtc.predict(x_test)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))

from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

"""# XGBOOST"""

from xgboost import XGBClassifier
classifier = XGBClassifier()
classifier.fit(x_train, y_train)

from sklearn.metrics import confusion_matrix, accuracy_score
y_pred = classifier.predict(x_test)
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

"""# **Deep Learning**

ANN
"""

import tensorflow as tf

ann = tf.keras.models.Sequential()

ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

ann.add(tf.keras.layers.Dense(units=6, activation='relu'))

ann.add(tf.keras.layers.Dense(units=5, activation='softmax'))

ann.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])

ann.fit(x_train, y_train, batch_size = 32, epochs = 100)

"""# Associative Mining"""

!pip install apyori

apriori_df

transactions = []
for i in range(0, 146072):
  transactions.append([str(apriori_df.values[i,j]) for j in range(0, 4)])

from apyori import apriori
rules = apriori(transactions = transactions, min_support = 0.003, min_confidence = 0.2, min_lift = 3, min_length = 2, max_length = 2)

results = list(rules)

results

def inspect(results):
    lhs         = [tuple(result[2][0][0])[0] for result in results]
    rhs         = [tuple(result[2][0][1])[0] for result in results]
    supports    = [result[1] for result in results]
    confidences = [result[2][0][2] for result in results]
    lifts       = [result[2][0][3] for result in results]
    return list(zip(lhs, rhs, supports, confidences, lifts))
resultsinDataFrame = pd.DataFrame(inspect(results), columns = ['Left Hand Side', 'Right Hand Side', 'Support', 'Confidence', 'Lift'])

resultsinDataFrame